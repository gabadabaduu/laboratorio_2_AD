{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj6HEh5w0GkgUAXs7FcIIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabadabaduu/laboratorio_2_AD/blob/main/crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Iv9PNK64WYCF"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "import json\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from queue import Queue\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_url_ok_to_follow(url, domain):\n",
        "    parsed_url = urlparse(url)\n",
        "    return (\n",
        "        parsed_url.scheme in (\"http\", \"https\") and\n",
        "        parsed_url.netloc.endswith(domain) and\n",
        "        \"@\" not in parsed_url.path and\n",
        "        parsed_url.path.endswith((\".html\", \"\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "pqt3L7s8rSnb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def go(n:int, dictionary:str, output:str):\n",
        "    visited_urls = set()\n",
        "    url_queue = Queue()\n",
        "    base_url = \"https://educacionvirtual.javeriana.edu.co/nuestros-programas-nuevo\"\n",
        "    domain = \"educacionvirtual.javeriana.edu.co\"\n",
        "    url_queue.put(base_url)\n",
        "    courses = []\n",
        "\n",
        "    while not url_queue.empty() and len(visited_urls) < n:\n",
        "        url = url_queue.get()\n",
        "        if url not in visited_urls and is_url_ok_to_follow(url, domain):\n",
        "            visited_urls.add(url)\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                # Obtener la información de los cursos\n",
        "                course_elements = soup.find_all('div', class_='course-info')\n",
        "                for course_element in course_elements:\n",
        "                    # Extraer los datos del curso\n",
        "                    course_name = course_element.find('h3').text.strip()\n",
        "                    course_link = url\n",
        "                    course_duration = course_element.find('span', class_='duration').text.strip()\n",
        "                    course_level = course_element.find('span', class_='level').text.strip()\n",
        "                    course_start_date = course_element.find('span', class_='start-date').text.strip()\n",
        "                    course_price = course_element.find('span', class_='price').text.strip()\n",
        "\n",
        "                    # Crear un diccionario con los datos del curso y agregarlo a la lista de cursos\n",
        "                    course_data = {\n",
        "                        \"id\": len(courses) + 1,\n",
        "                        \"nombre\": course_name,\n",
        "                        \"enlace\": course_link,\n",
        "                        \"duracion\": course_duration,\n",
        "                        \"nivel\": course_level,\n",
        "                        \"fecha_inicio\": course_start_date,\n",
        "                        \"precio\": course_price\n",
        "                    }\n",
        "                    courses.append(course_data)\n",
        "            else:\n",
        "                print(f\"Failed to fetch URL: {url}\")\n",
        "                continue\n",
        "\n",
        "            # Obtener enlaces de la página y agregarlos a la cola\n",
        "            links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "            for link in links:\n",
        "                url_queue.put(link)\n",
        "\n",
        "    # Guardar la lista de cursos en el archivo JSON\n",
        "    with open(dictionary, 'w') as json_file:\n",
        "        json.dump(courses, json_file, indent=4)\n",
        "\n",
        "go(10, \"cursos.json\", \"cursos.csv\")"
      ],
      "metadata": {
        "id": "Z8FkqpjGtXBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}