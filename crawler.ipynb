{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXAyAGCoUQ5RvEKoCR5lqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabadabaduu/laboratorio_2_AD/blob/main/crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv9PNK64WYCF"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "import json\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from queue import Queue\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_url_ok_to_follow(url, domain):\n",
        "    parsed_url = urlparse(url)\n",
        "    return (\n",
        "        parsed_url.scheme in (\"http\", \"https\") and\n",
        "        parsed_url.netloc.endswith(domain) and\n",
        "        \"@\" not in parsed_url.path and\n",
        "        parsed_url.path.endswith((\".html\", \"\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "pqt3L7s8rSnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def go(n:int, dictionary:str, output:str):\n",
        "    visited_urls = set()\n",
        "    url_queue = Queue()\n",
        "    base_url = \"https://educacionvirtual.javeriana.edu.co/nuestros-programas-nuevo\"\n",
        "    domain = \"educacionvirtual.javeriana.edu.co\"\n",
        "    url_queue.put(base_url)\n",
        "    courses = []\n",
        "\n",
        "    while not url_queue.empty() and len(visited_urls) < n:\n",
        "        url = url_queue.get()\n",
        "        if url not in visited_urls and is_url_ok_to_follow(url, domain):\n",
        "            visited_urls.add(url)\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                # Obtener la información de los cursos\n",
        "                course_elements = soup.find_all('div', class_='course-info')\n",
        "                for course_element in course_elements:\n",
        "                    # Extraer los datos del curso\n",
        "                    course_name = course_element.find('h3').text.strip()\n",
        "                    course_link = url\n",
        "                    course_duration = course_element.find('span', class_='duration').text.strip()\n",
        "                    course_level = course_element.find('span', class_='level').text.strip()\n",
        "                    course_start_date = course_element.find('span', class_='start-date').text.strip()\n",
        "                    course_price = course_element.find('span', class_='price').text.strip()\n",
        "\n",
        "                    # Crear un diccionario con los datos del curso y agregarlo a la lista de cursos\n",
        "                    course_data = {\n",
        "                        \"id\": len(courses) + 1,\n",
        "                        \"nombre\": course_name,\n",
        "                        \"enlace\": course_link,\n",
        "                        \"duracion\": course_duration,\n",
        "                        \"nivel\": course_level,\n",
        "                        \"fecha_inicio\": course_start_date,\n",
        "                        \"precio\": course_price\n",
        "                    }\n",
        "                    courses.append(course_data)\n",
        "            else:\n",
        "                print(f\"Failed to fetch URL: {url}\")\n",
        "                continue\n",
        "\n",
        "            # Obtener enlaces de la página y agregarlos a la cola\n",
        "            links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "            for link in links:\n",
        "                url_queue.put(link)\n",
        "\n",
        "    # Guardar la lista de cursos en el archivo JSON\n",
        "    with open(dictionary, 'w') as json_file:\n",
        "        json.dump(courses, json_file, indent=4)\n",
        "\n",
        "go(10, \"cursos.json\", \"cursos.csv\")"
      ],
      "metadata": {
        "id": "Z8FkqpjGtXBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://educacionvirtual.javeriana.edu.co/nuestros-programas-nuevo\"\n",
        "domain = \"educacionvirtual.javeriana.edu.co\"\n",
        "\n",
        "# Obtener el contenido HTML de la página\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Función para limpiar una palabra\n",
        "def clean_word(word):\n",
        "    # Eliminar signos de puntuación al final de la palabra\n",
        "    word = re.sub(r'[!.,:]+$', '', word)\n",
        "    return word.lower()\n",
        "\n",
        "# Función para procesar el texto y extraer palabras\n",
        "def process_text(text):\n",
        "    # Expresión regular para identificar palabras válidas\n",
        "    word_regex = re.compile(r'^[a-zA-ZáéíóúÁÉÍÓÚñÑ][a-zA-ZáéíóúÁÉÍÓÚñÑ0-9_]*$')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        cleaned_word = clean_word(word)\n",
        "        if word_regex.match(cleaned_word) and len(cleaned_word) > 1:\n",
        "            words.append(cleaned_word)\n",
        "    return words\n",
        "\n",
        "# Función para encontrar todas las palabras en el contenido de un curso\n",
        "def find_words(course_content):\n",
        "    words = set()\n",
        "    for tag in course_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "        words.update(process_text(tag.get_text()))\n",
        "    return words\n",
        "\n",
        "# Función para construir el índice de palabras para un curso\n",
        "def build_index(course_id, course_content, index, course_counter):\n",
        "    words = find_words(course_content)\n",
        "    for word in words:\n",
        "        index[f\"{course_id}_{course_counter:04d}\"] = word\n",
        "        course_counter += 1\n",
        "    return index, course_counter\n",
        "\n",
        "\n",
        "\n",
        "# Función para guardar el índice en un archivo CSV\n",
        "def save_index_to_csv(index, filename):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, delimiter='|')\n",
        "        for word, course_id in index.items():\n",
        "            writer.writerow([course_id, word])\n"
      ],
      "metadata": {
        "id": "3PxXazQxUr_v"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}